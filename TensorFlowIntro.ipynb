{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "We don't have to code up back propagation for every possible function or neural network architecture that we want to fit. There are lots of libraries targeted towards machine learning that make this task easy and computationally efficient. One of the most popular libraries is [TensorFlow](https://www.tensorflow.org/). It was developed by Google Brain and is now open source under the Apache License 2.0.\n",
    "\n",
    "The workflow consists of building a computational graph where \"operations\" act on \"tensors\" that can be automatically differentiated. The tensors themselves don't hold values, but instead are \"initialized\" or \"fed\" when actually running the computation. We will see how that works in this tutorial.\n",
    "\n",
    "The TensorFlow website contains a much more [detailed introduction](https://www.tensorflow.org/guide/low_level_intro) if you want to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto differentiation\n",
    "By building up a computiational graph, the gradient w.r.t. arbitrary parameters in the graph can be calculated via backpropagation. Let's try this for the toy example of calculating the first and second derivative of the sinus function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant(np.linspace(0, 2*np.pi, 100))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tf.sin(t)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.gradients(f, t)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the gradient is also just a computation graph, we can calculate the gradient of the gradient to get the second derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = tf.gradients(df, t)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.Tensor` objects above don't contain any values yet. We need to actually *run* the computation graph in a `tf.Session` to obtain the output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    t_np = sess.run(t)\n",
    "    plt.plot(t_np, sess.run(f), label=\"f(t)\")\n",
    "    plt.plot(t_np, sess.run(df[0]), label=\"f'(t)\")\n",
    "    plt.plot(t_np, sess.run(ddf[0]), label=\"f''(t)\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually build a NN in TensorFlow\n",
    "\n",
    "Let's build a 1-hidden-layer NN, similar to what we did in [NNFromScratch.ipynb](NNFromScratchNumpy.ipynb) now with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders are used for values that should be fed in later. Dimensions of size `None` are meant to be of arbitray size, in this case this will be the training example index. \n",
    "\n",
    "When we create Tensors, they will be added to the current graph of TensorFlow. To identify them later, in case we haven't assigned them to a python variable it is useful to give them a name. TensorFlow will attach an index to the name if it is already taken.\n",
    "\n",
    "The first Tensor will hold the input values that we will feed in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.placeholder(tf.float32, (None, 2), name='input')\n",
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the weights and biases for the hidden layer. We could use a placeholder as well and feed it the initial weights, but to illustrate a different concept, let's use a variable. We use the `tf.get_variable` method. Note that this method won't add indices to the names when they are already taken, but throw an exception instead.\n",
    "\n",
    "Variables have to be initialized. We could e.g. specify that the weights are initalized to some fixed values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.get_variable(\"W\", dtype=tf.float32, initializer=np.random.randn(2, 16).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.get_variable(\"b\", dtype=tf.float32, initializer=tf.zeros(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the output of the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.add(tf.matmul(inp, W), b, name=\"z\")\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.nn.relu(z, name=\"a\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the weights and outputs of the output layer.\n",
    "\n",
    "Let's try another method for variable initialization here - using tensorflows initializers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.get_variable(\"W2\", dtype=tf.float32, initializer=tf.random_normal_initializer()(shape=(16, 1)))\n",
    "b2 = tf.get_variable(\"b2\", dtype=tf.float32, initializer=tf.zeros(1))\n",
    "print(W2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2 = tf.add(tf.matmul(a, W2), b2)\n",
    "z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip the activation function, since we will use a loss function that already applies the sigmoid transformation. This is numerically more stable.\n",
    "\n",
    "But first, we need to define a Tensor that will hold the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, (None,1))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the binary cross entropy with a sigmoid transformation of the input values that don't have the sigmoid applied already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=z2)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now - **and this is the whole point of this tutorial** - to get the gradients w.r.t. all parameters, we can simply create the gradients as a `tf` operation and done!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad = tf.gradients(L, [W, b, W2, b2])\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `tensorboard` extension for Jupyter notebooks to display the graph we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.summary.writer.writer import FileWriter\n",
    "FileWriter('logs/train', graph=tf.get_default_graph()).close()\n",
    "%tensorboard --logdir logs/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the NN and calculate the gradient\n",
    "\n",
    "Getting the output of the NN and the gradient w.r.t. the parameters is now simply a matter of feeding in the values and running the actual `tf` graph. Lets create the feed values with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_val = np.random.randn(10, 2)\n",
    "inp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.random.randint(0, 2, size=10).reshape(-1, 1)\n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the graph, we have to create a `tf.Session` and pass a Tensor to the `run` method and a dictionary with the values for all placeholder tensors - in our case the input and targets. But before that, the values of the variables have to be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets just output the values of the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {inp : inp_val, y : y_val}\n",
    "grad_val = sess.run(\n",
    "    grad,\n",
    "    feed_dict=feed_dict\n",
    ")\n",
    "grad_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And store them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_tf, db_tf, dW2_tf, db2_tf = grad_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can reproduce that with the formulas we used in [NNFromScratch.ipynb](NNFromScratchNumpy.ipynb)\n",
    "\n",
    "Here a copy paste of the relevant functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    dZ = (Z >= 0)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.matmul(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        derivative_activation_func = relu_derivative\n",
    "    elif activation is \"sigmoid\":\n",
    "        derivative_activation_func = sigmoid_derivative\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "            \n",
    "    dZ_curr = dA_curr * derivative_activation_func(Z_curr)\n",
    "    dW_curr = np.matmul(\n",
    "        dZ_curr,\n",
    "        # need to transpose only the last 2 dimensions, \n",
    "        # since the first dimension is the training example index\n",
    "        np.transpose(A_prev, (0, 2, 1))\n",
    "    )\n",
    "    db_curr = dZ_curr\n",
    "    dA_prev = np.matmul(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_value(Y_hat, Y):\n",
    "    return - np.mean(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_derivative(Y_hat, Y):\n",
    "    return - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's calculate the forward pass. Unfortunately, tf and numpy have a bit different conventions for matmul, so i'm very sorry for all the confusing transposes and reshapes in the following section. If you have to do something like that, usually it's best to experiment and see if output dimensions are correct after each step.\n",
    "\n",
    "First, lets store the initialized values of the NN parameters in python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_val, b_val, W2_val, b2_val = sess.run([W, b, W2, b2], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run our manual `numpy` forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_val, z_val = single_layer_forward_propagation(inp_val.reshape(-1, 2, 1), W_val.T, b_val.reshape(-1, 1))\n",
    "print(a_val[0].reshape(-1))\n",
    "print(z_val[0].reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to what `tf` gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(a, feed_dict=feed_dict)[0])\n",
    "print(sess.run(z, feed_dict=feed_dict)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_val, z2_val = single_layer_forward_propagation(\n",
    "    a_val.reshape(-1, 16, 1), W2_val.T, b2_val.reshape(-1, 1), activation=\"sigmoid\"\n",
    ")\n",
    "print(a2_val.reshape(-1))\n",
    "print(z2_val.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `tf`, we don't have `a2` because we used a definition of the loss function where the sigmoid activation is already included. But we have `z2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(z2, feed_dict=feed_dict).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we implemented the forward pass correctly, so now lets do the backward pass and check if we get the same gradients like Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL = get_loss_derivative(a2_val, y_val.reshape(-1, 1, 1))\n",
    "dL.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propagate back into the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da, dW2, db2 = single_layer_backward_propagation(\n",
    "    dL, W2_val.T, b2_val.reshape(-1, 1), z2_val, a_val, activation=\"sigmoid\"\n",
    ")\n",
    "print(np.sum(dW2, axis=0).reshape(-1))\n",
    "print(np.sum(db2, axis=0).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dW2_tf.reshape(-1))\n",
    "print(db2_tf.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there into the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinp, dW, db = single_layer_backward_propagation(\n",
    "    da, W_val.T, b_val.reshape(-1, 1), z_val, inp_val.reshape(-1, 2, 1), activation=\"relu\"\n",
    ")\n",
    "print(np.sum(dW, axis=0).T)\n",
    "print(np.sum(db, axis=0).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dW_tf)\n",
    "print(db_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Tensorflow does the same thing we attempted to do before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
